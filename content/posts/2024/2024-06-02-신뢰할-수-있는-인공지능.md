---
title: 신뢰할 수 있는 인공지능
category:
  - 도서
---

ChatGPT가 세상을 뒤흔든 이후로 인공지능은 더 이상 연구실 안의 이야기가 아닌 일상이 됐습니다. 그런데 매일같이 쏟아지는 AI 뉴스를 보면서 늘 마음 한켠에 불편함이 남았습니다. "이 기술을 정말 믿어도 되는 걸까?" 한상기 저자의 『신뢰할 수 있는 인공지능』은 바로 그 불편함에 정면으로 답하는 책입니다.

<img src="https://image.yes24.com/goods/119671572/XL" width="400" alt="신뢰할 수 있는 인공지능"/>

저자 한상기는 카이스트에서 컴퓨터공학을 전공하고 테크프런티어 대표로 AI 정책과 산업을 오랫동안 분석해 온 전문가입니다. 이 책은 단순히 기술적 원리를 설명하는 책이 아니라, 인공지능을 둘러싼 사회적·윤리적·제도적 맥락을 함께 짚어주는 보기 드문 교양서입니다.

---

## 1. 인공지능과 신뢰성 — 왜 지금 이 질문인가

책의 첫 장은 도발적인 질문으로 시작합니다. "당신은 AI의 판단을 믿습니까?"

우리는 이미 수없이 많은 AI 판단에 의존하고 있습니다. 신용카드 이상 거래 탐지, 병원의 영상 진단 보조, 채용 서류 필터링, 재판 보석 여부 예측까지. 문제는 이런 결정들이 우리 삶에 직접적인 영향을 미침에도 불구하고, 그 판단 과정이 블랙박스처럼 불투명하다는 점입니다.

저자는 신뢰성(Trustworthiness)을 단일 속성으로 보지 않고 여러 차원이 결합된 개념으로 정의합니다. 기술적으로 정확한가, 예상치 못한 상황에서도 안전한가, 사람이 이해할 수 있는 방식으로 작동하는가, 윤리적으로 올바른가 — 이 모든 것이 맞물려야 비로소 신뢰할 수 있는 AI라 부를 수 있다는 것입니다.

> 신뢰는 정확도 수치가 아니라, 실패했을 때 어떻게 행동하는가에서 만들어진다.

이 한 문장이 책 전체의 방향을 잘 압축합니다. 99%의 정확도를 자랑하는 모델이라도 나머지 1%의 실패가 치명적이라면, 우리는 그 모델을 신뢰할 수 없습니다.

---

## 2. 윤리와 공정성 — AI가 차별한다

AI 편향(Bias)은 학계에서 오래 논의된 주제지만, 이 책은 사례 중심으로 이를 매우 실감나게 전달합니다.

가장 인상 깊었던 사례는 미국의 COMPAS 알고리즘입니다. 재범 위험도를 예측해 보석 여부를 결정하는 이 시스템은 흑인 피의자에게 더 높은 위험 점수를 부여하는 경향이 있었고, 이는 인종 차별을 알고리즘으로 제도화한 것이나 마찬가지라는 비판을 받았습니다. 놀라운 것은, 이 알고리즘이 인종 정보를 직접 사용하지 않았음에도 불구하고 이런 결과가 나왔다는 점입니다. 우편번호, 교육 수준, 소득 같은 '중립적' 변수들이 인종과 강하게 상관되어 있었기 때문입니다.

이것이 AI 공정성 문제의 핵심 딜레마입니다. 데이터는 사회의 산물이고, 사회에는 이미 불평등이 내재되어 있습니다. 과거 데이터를 학습한 AI는 그 불평등까지 학습하게 됩니다.

저자는 공정성(Fairness)의 수학적 정의가 여럿 존재하며, 이것들이 상호 충돌한다는 점도 지적합니다.

| 공정성 기준 | 설명 | 문제점 |
|------------|------|--------|
| 개인 공정성 | 유사한 개인에게 유사한 결과 | "유사함"의 정의가 어렵다 |
| 그룹 공정성 | 집단 간 결과 분포가 동일 | 개인 불공정이 생길 수 있다 |
| 기회 균등 | 집단 간 위양성률 동일 | 음성 예측 정확도는 다를 수 있다 |
| 균등 정확도 | 집단 간 정확도 동일 | 위양성·위음성 비율이 다를 수 있다 |

이 네 가지를 동시에 만족하는 것은 수학적으로 불가능합니다. 결국 어떤 공정성을 우선할지는 기술이 아니라 사회적 합의의 문제라는 것이 저자의 결론입니다.

---

## 3. 투명성과 설명 가능성 — "왜 그런 결론을 냈나요?"

딥러닝 모델은 강력하지만 불투명합니다. 수십억 개의 가중치가 얽혀 만들어내는 판단을 인간이 직관적으로 이해하기란 어렵습니다. 이 챕터는 그 문제를 어떻게 해결하려는지, XAI(Explainable AI) 분야의 다양한 시도를 소개합니다.

**LIME(Local Interpretable Model-agnostic Explanations)**은 복잡한 모델의 특정 예측 주변을 단순한 선형 모델로 근사해서, "이 입력의 어떤 부분이 이 판단에 영향을 줬는가"를 보여줍니다. 예를 들어 의료 이미지 분류 모델이 "폐렴 의심"이라고 판단했을 때, 어느 부위를 주목했는지 히트맵으로 시각화할 수 있습니다.

**SHAP(SHapley Additive exPlanations)**은 게임 이론의 샤플리 값에서 아이디어를 가져옵니다. 각 특성(feature)이 예측 결과에 기여한 정도를 수치로 나타내어, "당신의 대출이 거절된 가장 큰 이유는 소득 대비 부채 비율(DTI)이 높기 때문입니다"처럼 설명하는 것이 가능해집니다.

저자는 EU의 GDPR이 "자동화된 결정에 대한 설명 요구권"을 명시한 것을 언급하면서, 설명 가능성이 더 이상 연구자들의 관심사가 아니라 법적 의무가 되고 있음을 강조합니다.

그러나 설명 가능성에도 한계는 있습니다. 사후 설명(post-hoc explanation)은 실제 모델의 작동 방식이 아니라 근사일 뿐이며, 설명이 너무 단순화되면 오히려 사용자에게 잘못된 확신을 줄 수도 있습니다. 투명성과 성능 사이의 긴장은 아직 해소되지 않은 과제입니다.

---

## 4. 견고성과 안전성 — 의도하지 않은 실패와 의도적인 공격

AI 시스템이 예상치 못한 상황에서 얼마나 안정적으로 작동하는가, 그리고 악의적인 공격에 얼마나 저항할 수 있는가. 이 챕터는 기술적으로 가장 흥미로운 내용을 담고 있습니다.

**분포 이탈(Distribution Shift)**은 현실에서 가장 자주 마주치는 문제입니다. 맑은 날 도로 사진으로 학습한 자율주행 모델은 폭설이나 안개 상황에서 성능이 급격히 저하될 수 있습니다. 학습 환경과 실제 배포 환경이 다를 때, 모델은 조용히 실패합니다.

**적대적 공격(Adversarial Attack)**은 더 무서운 문제입니다. 사람 눈에는 전혀 차이가 없어 보이는 미세한 픽셀 변조만으로도 이미지 분류 모델을 완전히 속일 수 있습니다. 판다 사진에 노이즈를 추가했더니 모델이 긴팔원숭이라고 99% 확신으로 분류했다는 유명한 예시가 있습니다. 자율주행 차의 정지 표지판에 스티커를 붙여 속도 제한 표지로 인식하게 만들 수 있다는 연구 결과도 있습니다.

저자는 이에 대한 방어 전략들을 소개합니다.

- **적대적 훈련(Adversarial Training):** 의도적으로 공격 샘플을 훈련 데이터에 포함시켜 모델을 강화하는 방법
- **인증된 방어(Certified Defense):** 입력이 특정 범위 내에서 변조되더라도 예측이 바뀌지 않음을 수학적으로 보장하는 방법
- **앙상블 방어:** 여러 모델을 조합해 단일 모델 공격의 영향을 줄이는 방법

안전한 AI를 만드는 것은 단순히 성능 지표를 높이는 것과는 다른 차원의 작업입니다. 이 챕터를 읽으면서 AI 시스템을 설계할 때 "얼마나 정확한가"뿐 아니라 "어떤 방식으로 실패하는가"를 함께 고려해야 한다는 관점이 생겼습니다.

---

## 5. 미래 전략과 관련 기업 — 신뢰를 어떻게 경쟁력으로 만들 것인가

마지막 챕터는 조금 더 실용적이고 전략적인 시각을 제시합니다. 신뢰할 수 있는 AI가 단순한 윤리 구호가 아니라, 기업과 국가에게 실질적인 경쟁력이 될 수 있다는 주장입니다.

**규제 환경의 변화**를 먼저 짚습니다. EU AI Act는 AI 시스템을 위험도에 따라 분류하고, 고위험 AI에는 엄격한 투명성·설명 가능성·견고성 요건을 부과합니다. 의료, 금융, 채용, 사법 분야의 AI는 사실상 XAI와 감사(Audit) 기능이 의무화되는 방향입니다. 미국도 AI 집행명령(Executive Order on AI)을 통해 안전성과 신뢰성 기준 마련을 서두르고 있습니다.

**글로벌 기업들의 접근 방식**도 흥미롭습니다.

| 기업 | 신뢰 AI 접근 방식 |
|------|-----------------|
| Google | 책임 있는 AI 원칙 7가지 공개, 모델 카드(Model Card) 도입 |
| Microsoft | AI 책임 프레임워크, Azure에 공정성 측정 툴킷 내장 |
| IBM | AI Fairness 360, AI Explainability 360 오픈소스 공개 |
| Anthropic | 헌법 기반 AI(Constitutional AI)로 안전성 내재화 시도 |

저자는 한국 기업들에게도 시사점을 제시합니다. 글로벌 시장에서 경쟁하려면 기술 성능만큼이나 신뢰성 인증과 설명 가능성 대응이 필수 역량이 될 것이라는 전망입니다. "AI 윤리팀이 별도로 있는 것"이 아니라, 개발 과정 전반에 신뢰성 관점이 내재화되어야 한다고 강조합니다.

---

## 책을 덮으며

이 책을 읽으면서 가장 크게 바뀐 생각은, AI의 신뢰 문제가 기술자만의 문제가 아니라는 것입니다. 공정성을 어떻게 정의할지, 어떤 설명을 요구할지, 실패를 어디까지 허용할지는 결국 사회적 합의의 문제입니다. 기술은 그 합의를 구현하는 도구일 뿐입니다.

개발자로서 성능 지표와 마감에 쫓기다 보면 "이 모델이 어떻게 실패할 수 있는가"를 깊이 생각하지 못할 때가 많습니다. 이 책은 그 질문을 다시 꺼내 들게 합니다. 기술적으로 정확한 AI보다 사람이 믿을 수 있는 AI를 만드는 것이 더 어렵고, 동시에 더 중요한 일이라는 것을.

AI를 개발하거나 도입하는 조직에 있다면, 기술 문서만큼이나 이 책을 권하고 싶습니다.

---

## References

- 한상기, 『신뢰할 수 있는 인공지능』, 클라우드나인, 2023
- [EU AI Act 전문](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206)
- [Google AI 원칙](https://ai.google/responsibility/principles/)
- [IBM AI Fairness 360](https://aif360.mybluemix.net/)
- [LIME 논문: "Why Should I Trust You?" (Ribeiro et al., 2016)](https://arxiv.org/abs/1602.04938)
- [SHAP 논문 (Lundberg & Lee, 2017)](https://arxiv.org/abs/1705.07874)
