---
title: LLM을 활용한 실전 AI 어플리케이션 개발
category:
  - AI
---

허정준 님의 강연 "LLM을 활용한 실전 AI 어플리케이션 개발"을 듣고 정리한 내용입니다. LLM의 기초 아키텍처부터 실제 서비스 운영까지, 넓은 범위를 체계적으로 다루는 강연이었습니다.

---

## 1. LLM 지도 — 전체 그림부터

LLM을 제대로 활용하려면 관련 기술들이 어떻게 연결되는지 전체 지형도를 먼저 이해하는 게 중요합니다.

### 딥러닝과 언어 모델

언어 모델의 핵심 역할은 단순합니다. **다음에 올 단어를 예측하는 것**입니다. 이 단순한 목표가 엄청난 양의 텍스트 데이터와 결합되면서 오늘날의 LLM이 탄생했습니다.

기술 계층을 정리하면 다음과 같습니다.

| 구분 | 설명 |
|------|------|
| 머신러닝 | 입력 → 특징 추출 → 분류 → 출력 |
| 딥러닝 | 입력 → 특징 추출 + 분류(통합) → 출력 |
| 임베딩 | 데이터의 의미와 특징을 포착해 숫자로 표현 |
| 전이 학습 | 하나의 문제를 해결하면서 얻은 지식을 다른 문제에 활용 |

### 효율적인 학습과 추론을 위한 핵심 기술

LLM의 실용화를 가능하게 한 기술들입니다.

- **sLLM**: 모델 크기는 작지만 특정 도메인에서 높은 성능을 내는 소형 언어 모델
- **양자화(Quantization)**: 모델 파라미터를 더 작은 비트로 표현해 메모리 사용량을 줄임
- **LoRA(Low Rank Adaptation)**: 모델의 일부 파라미터만 학습해 효율적으로 미세 조정
- **RAG(Retrieval Augmented Generation)**: 프롬프트에 LLM이 답변할 데이터를 미리 추가해 최신성과 정확도를 높임
- **멀티모달**: 텍스트를 넘어 이미지, 음성 등 여러 형태의 데이터를 입출력
- **에이전트**: LLM이 계획을 세우고 의사결정을 내리며 필요한 행동까지 수행

---

## 2. 트랜스포머 아키텍처

### RNN의 한계

트랜스포머 이전에는 RNN이 시계열·시퀀스 데이터를 처리했습니다. RNN은 지금까지 입력된 텍스트의 맥락을 하나의 **잠재 상태(hidden state)**에 압축합니다. 그러나 레이어가 깊어질수록 과거 정보가 희석되는 **Vanishing Gradient** 문제가 있었고, LSTM으로 이를 보완했습니다.

> RNN/LSTM은 ReLU 대신 softmax 또는 tanh를 활성화 함수로 사용합니다.

### 트랜스포머의 등장

트랜스포머는 RNN의 순차적 처리 방식을 버리고, 입력 전체를 동시에 참조하는 **self-attention** 연산을 채택했습니다. 여러 어텐션을 동시에 적용하는 **multi-head attention**으로 더 높은 성능을 냅니다.

매 단어를 예측할 때마다 전체 맥락을 확인하므로 연산량은 늘었지만, 성능은 비약적으로 향상됐습니다.

### 주요 아키텍처 분류

| 아키텍처 | 대표 모델 | 특징 |
|----------|-----------|------|
| 인코더 | BERT (Google) | 마스크 언어 모델링, 양방향 문맥 이해 |
| 디코더 | GPT (OpenAI) | 인과적 언어 모델링(CLM), 단방향, 생성 특화 |
| 인코더+디코더 | BART (Meta), T5 (Google) | 번역·요약 등 시퀀스-투-시퀀스 태스크에 강점 |

### 허깅페이스 트랜스포머

모델마다 구현 방식이 달라 각기 활용법을 익혀야 했던 문제를 **허깅페이스 트랜스포머 라이브러리**가 공통 인터페이스로 해결했습니다. 모델 허브, 토크나이저, 데이터셋, 학습용 `Trainer`, 추론용 `pipeline`을 통합 제공합니다.

```bash
pip install transformers==4.40.1 datasets==2.19.0 huggingface_hub==0.23.0
```

---

## 3. 말 잘 듣는 모델 만들기

### 지도 미세 조정(SFT)

사전 학습된 언어 모델은 텍스트를 잘 생성하지만 사용자 요청에 적절히 응답하는 방식은 따로 학습해야 합니다. **지도 미세 조정(Supervised Fine-Tuning)**은 요청-응답 쌍이 담긴 **지시 데이터셋(instruction dataset)**으로 이를 보완합니다.

대표적인 예가 스탠퍼드 대학교의 **알파카(Alpaca)** 프로젝트입니다. 오픈소스 LLaMA 모델을 `instruction`, `input`, `output`으로 구성된 템플릿으로 추가 학습했습니다.

### 강화 학습을 통한 정렬

모델을 사람의 의도에 맞게 정렬(alignment)하는 방법이 여러 가지 발전해 왔습니다.

- **RLHF**: 사람의 피드백을 보상 신호로 활용하는 강화 학습. 단, **보상 해킹(reward hacking)** 문제가 있음
- **PPO(Proximal Policy Optimization)**: 가까운 범위에서 높은 보상을 탐색하는 근접 정책 최적화
- **기각 샘플링(Rejection Sampling)**: SFT 모델로 여러 응답을 생성하고, 리워드 모델이 가장 높은 점수를 준 것만 다시 SFT에 사용
- **DPO(Direct Preference Optimization)**: 강화 학습 없이 선호 데이터셋을 직접 학습. RLHF보다 단순하면서 효과적이고 리워드 모델이 필요 없음

---

## 4. GPU 효율적인 학습

### 수치 표현과 양자화

부동소수점 정밀도를 줄이면 메모리를 절약할 수 있습니다.

```
fp32 (지수 8bit, 가수 23bit)
  → fp16 (지수 5bit, 가수 10bit)
  → bf16 (지수 8bit, 가수 7bit)
```

**양자화**는 더 적은 비트를 사용하면서도 원본 정보 손실을 최소화하는 기술입니다.

### GPU 학습 최적화 기법

| 기법 | 설명 |
|------|------|
| Gradient Accumulation | 제한된 메모리에서 큰 배치 효과를 얻는 방법 |
| Gradient Checkpointing | 순전파 중간 값을 선별 저장해 메모리를 줄이고 필요 시 재계산 |
| 분산 학습 | 2개 이상의 GPU로 모델을 학습 |

### LoRA와 QLoRA

**PEFT(Parameter Efficient Fine-Tuning)**의 대표 기법인 **LoRA**는 모델에 소수의 파라미터를 추가하고 그 부분만 학습합니다. 여기에 양자화를 결합한 **QLoRA**는 메모리 효율을 한층 더 높입니다.

---

## 5. 모델 가볍게 만들기

### 양자화 라이브러리와 포맷

실제로 모델을 경량화할 때 쓰는 주요 도구들입니다.

- **bitsandbytes**: 워싱턴대학교에서 개발한 양자화 라이브러리
- **GPTQ(GPT Quantization)**: 사후 양자화 방식
- **AWQ(Activation-aware Weight Quantization)**: 활성화 값을 고려한 가중치 양자화
- **GGUF(Georgi Gerganov Unified Format)**: 다양한 칩(GPU, CPU, 애플 실리콘)에서 추론 가능한 모델 저장 형식 → 온디바이스 AI에 특히 유용

### 지식 증류(Knowledge Distillation)

더 크고 성능이 높은 **교사 모델(teacher model)**의 출력을 활용해 더 작은 **학생 모델(student model)**을 학습시키는 방법입니다. 작은 모델로도 큰 모델에 근접한 성능을 내는 것이 목표입니다.

---

## 6. sLLM 서빙하기

### 배치 전략

| 방식 | 특징 |
|------|------|
| 정적 배치 | 고정된 입력 묶음을 한 번에 처리 |
| 동적 배치 | 요청이 들어오는 대로 묶음 크기를 조절 |
| 연속 배치 | 처리 완료된 요청을 즉시 새 요청으로 교체해 GPU 활용도 극대화 |

### 효율적인 추론 기술

- **KV Cache**: 동일 연산의 반복을 줄이기 위해 계산 결과를 저장
- **플래시어텐션(FlashAttention)**: HBM(고대역폭 메모리) 접근을 최소화해 어텐션 연산을 가속
- **페이지어텐션(PagedAttention)**: KV Cache를 OS의 가상 메모리처럼 관리해 메모리 낭비를 줄임
- **커널 퓨전**: 여러 연산을 하나의 GPU 커널로 묶어 오버헤드 감소
- **추측 디코딩(Speculative Decoding)**: 소형 모델이 먼저 초안을 생성하고 대형 모델이 검증

### vLLM

LLM 서빙의 사실상 표준이 된 **vLLM** 프레임워크는 오프라인(배치 추론)과 온라인(실시간 서빙) 모두 지원합니다.

```bash
python -m vllm.entrypoints.openai.api_server \
  --model shangrilar/yi-ko-6b-text2sql \
  --host 127.0.0.1 \
  --port 8888 \
  --max-model-len 1024
```

---

## 7. LLM 어플리케이션 개발

### 오케스트레이션 도구

사용자 인터페이스, 임베딩 모델, 벡터 데이터베이스 등 다양한 구성 요소를 연결하는 프레임워크입니다.

- **LlamaIndex**: 데이터 인덱싱과 검색에 강점
- **LangChain**: 다양한 LLM과 도구를 체이닝
- **Canopy**: 대화형 애플리케이션 특화

### 벡터 데이터베이스

임베딩 벡터를 저장하고, 입력 벡터와 유사한 벡터를 빠르게 찾아주는 데이터베이스입니다. 유사도 측정에는 **유클리드 거리** 또는 **코사인 유사도**를 활용합니다.

| 구분 | 제품 |
|------|------|
| 오픈소스 | Chroma, Milvus, Qdrant |
| 상업용 | Pinecone, Weaviate |
| 벡터 기능 추가 DB | ElasticSearch, PostgreSQL, MongoDB, Neo4j |

### 데이터 검증과 로깅

- **검증 방법**: 규칙 기반 → 분류/회귀 모델 → 임베딩 유사도 → LLM 활용 순으로 복잡도가 높아짐
- **NeMo-Guardrails**: 엔비디아가 개발한 특정 질문 응답 회피 라이브러리
- **로깅 도구**: W&B, MLflow, PromptLayer

---

## 8. 임베딩 모델 깊이 보기

### 문장 임베딩 방식 비교

| 방식 | 원리 | 장점 | 단점 |
|------|------|------|------|
| 교차 인코더 | 두 문장을 함께 입력해 직접 비교 | 정확도 높음 | 느리고 확장성 낮음 |
| 바이 인코더 | 각 문장을 독립적으로 임베딩 후 거리 계산 | 빠르고 확장성 높음 | 상대적으로 낮은 정확도 |

### 검색 성능을 높이는 파이프라인

두 방식을 결합하면 속도와 정확도를 모두 잡을 수 있습니다.

```
검색 쿼리 → 바이 인코더 → 의미 검색 Top 100 → 교차 인코더 → Re-rank → Top K
```

### 임베딩 모델 발전 계보

원핫 인코딩 → Bag of Words → TF-IDF → **Word2Vec**(CBOW, Skip-gram) → 트랜스포머 기반 임베딩

**대조 학습(Contrastive Learning)**은 유사한 데이터는 가깝게, 유사하지 않은 데이터는 멀게 임베딩 공간을 구성하도록 학습하는 방법으로, 고품질 임베딩 모델 학습의 핵심 기법입니다.

---

## 9. 벡터 데이터베이스 내부 작동 원리

대규모 벡터 데이터베이스에서는 정확도보다 속도를 우선하는 **ANN(Approximate Nearest Neighbor)** 알고리즘을 사용합니다.

- **KNN**: 정확하지만 느림 (전수 검색)
- **IVF(Inverted File Index)**: 클러스터로 나눠 검색 범위를 줄임
- **HNSW(Hierarchical Navigable Small World)**: 그래프 기반 ANN으로 높은 성능과 빠른 검색 속도를 동시에 달성

---

## 10. LLMOps — 운영의 관점

기존 MLOps와 LLMOps의 차이는 크게 두 가지입니다.

1. **모델 크기**: LLM은 기존 모델보다 훨씬 크고, 상당수가 API 기반 상업용 모델
2. **평가 어려움**: 분류·회귀처럼 명확한 지표가 없어 결과물을 정량화하기 어려움

### LLM 평가 방법

| 방법 | 예시 지표 |
|------|-----------|
| 정량적 지표 | BLEU, ROUGE, PPL(Perplexity) |
| 사람 평가 | 직접 품질 평가 |
| LLM 평가 | GPT-4 등으로 자동 채점 |

### RAG 평가 3요소

- **신뢰성(Faithfulness)**: 답변이 검색된 맥락에 근거하는가
- **답변 관련성(Answer Relevancy)**: 답변이 질문에 관련 있는가
- **맥락 관련성(Context Relevancy)**: 검색된 맥락이 질문에 관련 있는가

---

## 11. 멀티모달 LLM

### 주요 모델

| 구분 | 모델 |
|------|------|
| 상업용 | GPT-4V, GPT-4o, Gemini |
| 오픈소스 | LLaVA, Fuyu-8B |

### 구성 요소

```
이미지 입력
  → 모달리티 인코더 (예: CLIP)
  → 입력 프로젝터 (이미지 임베딩 → LLM이 이해하는 텍스트 형식으로 변환)
  → LLM 백본
  → 출력 프로젝터
  → 모달리티 생성기
```

- **CLIP**: OpenAI가 개발한 이미지 인코더로 이미지를 텍스트와 같은 임베딩 공간에 배치
- **DALL-E**: CLIP의 반대 방향인 텍스트 → 이미지 생성 모델
- **LLaVA**: 이미지를 인식하고 그에 대한 텍스트를 생성하는 오픈소스 멀티모달 모델

---

## 12. LLM 에이전트

### 에이전트의 구성

에이전트는 알아서 생각하고 행동하는 시스템입니다. 세 가지 핵심 요소로 구성됩니다.

| 요소 | 내용 |
|------|------|
| 감각(Perception) | 멀티모달 입력으로 환경 인식 |
| 두뇌(Brain) | 이해, 기억, 지식, 계획 |
| 행동(Action) | 검색 API 호출, 코드 실행, 이미지/음성 생성, 번역/요약 등 |

### 에이전트 시스템 형태

- **단일 에이전트**: 하나의 에이전트가 모든 작업 담당
- **멀티 에이전트**: 각 에이전트에 서로 다른 역할(profile)을 부여해 협력

**구현 프레임워크**: AutoGen, MetaGPT, CrewAI 등

### 에이전트 평가 기준

아직 초기 단계라 평가 방법론이 정립되지 않았으나, 일반적으로 사람의 주관적 평가와 테스트 데이터 기반 객관적 평가를 병행합니다.

- 유용성
- 사회성
- 가치관
- 진화 능력

---

## 13. 새로운 아키텍처 — 맘바(Mamba)

2017년 트랜스포머가 등장한 이후 딥러닝의 핵심 아키텍처로 자리잡았지만, 2023년 **맘바(Mamba)** 아키텍처가 등장하며 주목받고 있습니다.

맘바는 트랜스포머 대비 **5배 빠른 추론 속도**를 주장합니다.

```
맘바 = SSM(State Space Model) + 선택 메커니즘
```

- **SSM**: 속도를 높이기 위한 전략으로, RNN을 개선한 구조
- **선택 메커니즘**: 문장의 맥락을 효율적으로 압축해 성능을 높이는 전략

---

## 정리하며

이 강연은 LLM의 이론적 토대부터 실전 운영까지 전체 스택을 하나의 흐름으로 연결한다는 점이 인상적이었습니다. 특히 "모델을 잘 만드는 것"과 "모델을 잘 쓰는 것"이 별개의 문제임을 명확히 해 줬습니다.

기술이 빠르게 발전하는 분야인 만큼 개별 도구보다는 각 레이어가 왜 필요한지, 어떤 문제를 해결하는지를 이해하는 것이 중요하다는 점을 다시 한번 느꼈습니다.
