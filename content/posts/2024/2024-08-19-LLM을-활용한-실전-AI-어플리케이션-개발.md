---
title: LLM을 활용한 실전 AI 어플리케이션 개발
category:
  - 도서
---

![](https://contents.kyobobook.co.kr/sih/fit-in/458x0/pdt/9791189909703.jpg)

허정준 지음, 박재호 감수의 『LLM을 활용한 실전 AI 애플리케이션 개발』(책만, 2024)을 읽고 정리한 감상입니다. 2025 대한민국학술원 우수학술도서로 선정된 이 책은 556쪽에 걸쳐 LLM의 기초 아키텍처부터 실전 서비스 운영까지 전 과정을 체계적으로 다룹니다.

---

## 책 한 줄 평

> LLM을 "만드는 것"과 "쓰는 것" 모두를 아우르는, 국내 저자가 쓴 거의 유일한 LLM 풀스택 안내서.

---

## 구성과 흐름

책은 4부 16장으로 구성됩니다. 1부에서 이론 기반을 닦고, 2부에서 모델을 직접 다루는 방법을 배우고, 3부에서 실전 애플리케이션을 만들고, 4부에서 멀티모달과 에이전트 같은 고급 주제로 마무리합니다. 읽는 내내 "왜 이 기술이 필요한가"라는 질문에 먼저 답한 뒤 세부 구현으로 들어가는 흐름이 자연스러웠습니다.

---

## 1부 — LLM의 기초 뼈대 세우기

### 1장. LLM 지도 — 전체 그림부터

책이 처음부터 전체 지형도를 제시해 주는 점이 좋았습니다. LLM을 제대로 활용하려면 관련 기술들이 어떻게 연결되는지 큰 그림을 먼저 잡아야 한다는 저자의 시각이 고스란히 드러납니다.

| 구분 | 설명 |
|------|------|
| 머신러닝 | 입력 → 특징 추출 → 분류 → 출력 |
| 딥러닝 | 입력 → 특징 추출 + 분류(통합) → 출력 |
| 임베딩 | 데이터의 의미와 특징을 포착해 숫자로 표현 |
| 전이 학습 | 하나의 문제를 해결하면서 얻은 지식을 다른 문제에 활용 |

LLM의 실용화를 가능하게 한 핵심 기술들을 초반에 정의해 두는 방식도 유용했습니다.

- **sLLM**: 모델 크기는 작지만 특정 도메인에서 높은 성능을 내는 소형 언어 모델
- **양자화(Quantization)**: 모델 파라미터를 더 작은 비트로 표현해 메모리 사용량을 줄임
- **LoRA(Low Rank Adaptation)**: 모델의 일부 파라미터만 학습해 효율적으로 미세 조정
- **RAG(Retrieval Augmented Generation)**: 프롬프트에 LLM이 답변할 데이터를 미리 추가해 최신성과 정확도를 높임
- **멀티모달**: 텍스트를 넘어 이미지, 음성 등 여러 형태의 데이터를 입출력
- **에이전트**: LLM이 계획을 세우고 의사결정을 내리며 필요한 행동까지 수행

### 2장. 트랜스포머 아키텍처

RNN의 한계에서 출발해 트랜스포머가 왜 등장했는지 설명하는 흐름이 설득력 있습니다. RNN은 지금까지 입력된 텍스트의 맥락을 하나의 **잠재 상태(hidden state)**에 압축하는데, 레이어가 깊어질수록 과거 정보가 희석되는 **Vanishing Gradient** 문제가 있었습니다.

트랜스포머는 이 순차적 처리 방식을 버리고 입력 전체를 동시에 참조하는 **self-attention** 연산을 채택했습니다. 주요 아키텍처 분류도 직관적으로 정리되어 있어 처음 접하는 독자도 이해하기 쉽습니다.

| 아키텍처 | 대표 모델 | 특징 |
|----------|-----------|------|
| 인코더 | BERT (Google) | 마스크 언어 모델링, 양방향 문맥 이해 |
| 디코더 | GPT (OpenAI) | 인과적 언어 모델링(CLM), 단방향, 생성 특화 |
| 인코더+디코더 | BART (Meta), T5 (Google) | 번역·요약 등 시퀀스-투-시퀀스 태스크에 강점 |

### 3장. 허깅페이스 트랜스포머 라이브러리

모델마다 구현 방식이 달라 각기 활용법을 익혀야 했던 문제를 **허깅페이스 트랜스포머 라이브러리**가 공통 인터페이스로 해결한 과정을 잘 설명합니다. 모델 허브, 토크나이저, 데이터셋, 학습용 `Trainer`, 추론용 `pipeline`을 아우르는 생태계를 실습 예제와 함께 익힐 수 있습니다.

### 4장. 말 잘 듣는 모델 만들기

사전 학습된 언어 모델이 텍스트를 잘 생성하더라도 사용자 요청에 적절히 응답하는 방식은 따로 학습해야 한다는 사실을 이 장에서 배웠습니다. **지도 미세 조정(SFT)**과 정렬 기법들을 비교한 부분이 특히 유익했습니다.

- **RLHF**: 사람의 피드백을 보상 신호로 활용하는 강화 학습. 단, **보상 해킹(reward hacking)** 문제가 있음
- **PPO(Proximal Policy Optimization)**: 가까운 범위에서 높은 보상을 탐색하는 근접 정책 최적화
- **기각 샘플링(Rejection Sampling)**: SFT 모델로 여러 응답을 생성하고, 리워드 모델이 가장 높은 점수를 준 것만 다시 SFT에 사용
- **DPO(Direct Preference Optimization)**: 강화 학습 없이 선호 데이터셋을 직접 학습. RLHF보다 단순하면서 효과적이고 리워드 모델이 필요 없음

---

## 2부 — LLM 들이기

### 5장. GPU 효율적인 학습

부동소수점 정밀도 개념부터 설명한 뒤 메모리 절약 기법으로 자연스럽게 이어지는 구성이 탄탄합니다.

```
fp32 (지수 8bit, 가수 23bit)
  → fp16 (지수 5bit, 가수 10bit)
  → bf16 (지수 8bit, 가수 7bit)
```

| 기법 | 설명 |
|------|------|
| Gradient Accumulation | 제한된 메모리에서 큰 배치 효과를 얻는 방법 |
| Gradient Checkpointing | 순전파 중간 값을 선별 저장해 메모리를 줄이고 필요 시 재계산 |
| 분산 학습 | 2개 이상의 GPU로 모델을 학습 |

**PEFT(Parameter Efficient Fine-Tuning)**의 대표 기법인 **LoRA**는 모델에 소수의 파라미터를 추가하고 그 부분만 학습합니다. 여기에 양자화를 결합한 **QLoRA**는 메모리 효율을 한층 더 높입니다. 개인 장비에서도 파인튜닝이 가능하다는 점을 실감할 수 있었습니다.

### 6장. sLLM 학습하기

직접 소형 언어 모델을 학습시키는 과정을 다룹니다. 이론으로만 알던 내용을 코드로 실습할 수 있어 이해가 깊어지는 챕터입니다.

### 7장. 모델 가볍게 만들기

실제로 모델을 경량화할 때 쓰는 주요 도구들을 한데 정리해 둔 점이 실용적입니다.

- **bitsandbytes**: 워싱턴대학교에서 개발한 양자화 라이브러리
- **GPTQ(GPT Quantization)**: 사후 양자화 방식
- **AWQ(Activation-aware Weight Quantization)**: 활성화 값을 고려한 가중치 양자화
- **GGUF(Georgi Gerganov Unified Format)**: 다양한 칩(GPU, CPU, 애플 실리콘)에서 추론 가능한 모델 저장 형식 → 온디바이스 AI에 특히 유용

**지식 증류(Knowledge Distillation)** 기법도 설명합니다. 더 크고 성능이 높은 **교사 모델(teacher model)**의 출력을 활용해 더 작은 **학생 모델(student model)**을 학습시키는 방법으로, 온디바이스 AI 트렌드와도 맞닿아 있어 흥미롭게 읽었습니다.

### 8장. sLLM 서빙하기

서빙은 개발자 입장에서 가장 현실적인 고민이 담긴 챕터입니다. 배치 전략부터 메모리 관리까지 실무에 바로 적용할 수 있는 내용이 많았습니다.

| 방식 | 특징 |
|------|------|
| 정적 배치 | 고정된 입력 묶음을 한 번에 처리 |
| 동적 배치 | 요청이 들어오는 대로 묶음 크기를 조절 |
| 연속 배치 | 처리 완료된 요청을 즉시 새 요청으로 교체해 GPU 활용도 극대화 |

- **KV Cache**: 동일 연산의 반복을 줄이기 위해 계산 결과를 저장
- **플래시어텐션(FlashAttention)**: HBM(고대역폭 메모리) 접근을 최소화해 어텐션 연산을 가속
- **페이지어텐션(PagedAttention)**: KV Cache를 OS의 가상 메모리처럼 관리해 메모리 낭비를 줄임
- **추측 디코딩(Speculative Decoding)**: 소형 모델이 먼저 초안을 생성하고 대형 모델이 검증

LLM 서빙의 사실상 표준이 된 **vLLM** 프레임워크를 직접 실행해 볼 수 있는 예제도 포함되어 있습니다.

```bash
python -m vllm.entrypoints.openai.api_server \
  --model shangrilar/yi-ko-6b-text2sql \
  --host 127.0.0.1 \
  --port 8888 \
  --max-model-len 1024
```

---

## 3부 — LLM을 활용한 실전 애플리케이션 개발

책에서 가장 실용적인 부분입니다. RAG 파이프라인을 처음부터 끝까지 직접 구축해 볼 수 있도록 안내합니다.

### 9장. LLM 애플리케이션 개발하기

사용자 인터페이스, 임베딩 모델, 벡터 데이터베이스 등 다양한 구성 요소를 연결하는 오케스트레이션 프레임워크를 비교합니다.

- **LlamaIndex**: 데이터 인덱싱과 검색에 강점
- **LangChain**: 다양한 LLM과 도구를 체이닝
- **Canopy**: 대화형 애플리케이션 특화

벡터 데이터베이스 비교도 간결하게 정리되어 있어 프로젝트에 맞는 도구를 고를 때 참고하기 좋습니다.

| 구분 | 제품 |
|------|------|
| 오픈소스 | Chroma, Milvus, Qdrant |
| 상업용 | Pinecone, Weaviate |
| 벡터 기능 추가 DB | ElasticSearch, PostgreSQL, MongoDB, Neo4j |

### 10장. 임베딩 모델로 데이터 의미 압축하기

임베딩 모델의 발전 계보와 두 가지 핵심 방식을 비교한 내용이 인상적이었습니다.

| 방식 | 원리 | 장점 | 단점 |
|------|------|------|------|
| 교차 인코더 | 두 문장을 함께 입력해 직접 비교 | 정확도 높음 | 느리고 확장성 낮음 |
| 바이 인코더 | 각 문장을 독립적으로 임베딩 후 거리 계산 | 빠르고 확장성 높음 | 상대적으로 낮은 정확도 |

두 방식을 결합하면 속도와 정확도를 모두 잡을 수 있습니다.

```
검색 쿼리 → 바이 인코더 → 의미 검색 Top 100 → 교차 인코더 → Re-rank → Top K
```

원핫 인코딩 → Bag of Words → TF-IDF → **Word2Vec**(CBOW, Skip-gram) → 트랜스포머 기반 임베딩으로 이어지는 계보도 깔끔하게 정리되어 있습니다.

### 11장. 자신의 데이터에 맞춘 임베딩 모델 만들기

**대조 학습(Contrastive Learning)**을 활용해 도메인 특화 임베딩 모델을 만드는 방법을 다룹니다. RAG 성능을 끌어올리고 싶다면 꼭 읽어야 할 챕터입니다.

### 12장. 벡터 데이터베이스로 확장하기

대규모 벡터 데이터베이스에서는 정확도보다 속도를 우선하는 **ANN(Approximate Nearest Neighbor)** 알고리즘이 쓰인다는 사실을 이 장에서 처음 제대로 이해했습니다.

- **KNN**: 정확하지만 느림 (전수 검색)
- **IVF(Inverted File Index)**: 클러스터로 나눠 검색 범위를 줄임
- **HNSW(Hierarchical Navigable Small World)**: 그래프 기반 ANN으로 높은 성능과 빠른 검색 속도를 동시에 달성

### 13장. LLM 운영하기

**LLMOps**는 기존 MLOps와 두 가지 면에서 다릅니다. 모델 크기가 훨씬 크고 상당수가 API 기반 상업용 모델이라는 점, 그리고 분류·회귀처럼 명확한 지표가 없어 평가가 어렵다는 점입니다.

| 방법 | 예시 지표 |
|------|-----------|
| 정량적 지표 | BLEU, ROUGE, PPL(Perplexity) |
| 사람 평가 | 직접 품질 평가 |
| LLM 평가 | GPT-4 등으로 자동 채점 |

RAG 평가 3요소도 실무에서 바로 쓸 수 있는 기준입니다.

- **신뢰성(Faithfulness)**: 답변이 검색된 맥락에 근거하는가
- **답변 관련성(Answer Relevancy)**: 답변이 질문에 관련 있는가
- **맥락 관련성(Context Relevancy)**: 검색된 맥락이 질문에 관련 있는가

---

## 4부 — 멀티모달, 에이전트 그리고 LLM의 미래

### 14장. 멀티모달

| 구분 | 모델 |
|------|------|
| 상업용 | GPT-4V, GPT-4o, Gemini |
| 오픈소스 | LLaVA, Fuyu-8B |

```
이미지 입력
  → 모달리티 인코더 (예: CLIP)
  → 입력 프로젝터 (이미지 임베딩 → LLM이 이해하는 텍스트 형식으로 변환)
  → LLM 백본
  → 출력 프로젝터
  → 모달리티 생성기
```

- **CLIP**: OpenAI가 개발한 이미지 인코더로 이미지를 텍스트와 같은 임베딩 공간에 배치
- **DALL-E**: CLIP의 반대 방향인 텍스트 → 이미지 생성 모델
- **LLaVA**: 이미지를 인식하고 그에 대한 텍스트를 생성하는 오픈소스 멀티모달 모델

### 15장. LLM 에이전트

에이전트를 구성하는 세 요소가 명쾌하게 정리되어 있습니다.

| 요소 | 내용 |
|------|------|
| 감각(Perception) | 멀티모달 입력으로 환경 인식 |
| 두뇌(Brain) | 이해, 기억, 지식, 계획 |
| 행동(Action) | 검색 API 호출, 코드 실행, 이미지/음성 생성, 번역/요약 등 |

**멀티 에이전트** 시스템에서 각 에이전트에 서로 다른 역할(profile)을 부여해 협력하는 방식은 책이 출간된 이후로도 빠르게 발전하고 있는 분야라 더욱 흥미롭게 읽었습니다. AutoGen, MetaGPT, CrewAI 등 구현 프레임워크도 간략히 소개합니다.

### 16장. 새로운 아키텍처 — 맘바(Mamba)

2017년 트랜스포머가 등장한 이후 그 자리를 위협한 **맘바(Mamba)** 아키텍처로 책이 마무리됩니다.

```
맘바 = SSM(State Space Model) + 선택 메커니즘
```

트랜스포머 대비 **5배 빠른 추론 속도**를 주장하며 등장했지만, 현재(2026년)까지도 트랜스포머 계열 모델이 주류를 이루고 있다는 점에서 새 아키텍처의 자리잡기가 얼마나 어려운지 실감하게 됩니다.

---

## 읽고 난 소감

이 책이 좋았던 이유는 단순히 지식을 나열하지 않기 때문입니다. 각 챕터가 "왜 이 기술이 필요한가"라는 질문에 먼저 답하고, 그 맥락 위에서 구현 세부 사항을 설명합니다. 덕분에 개별 기술들이 고립된 지식이 아니라 하나의 큰 흐름 속에서 자연스럽게 자리를 잡습니다.

특히 인상 깊었던 점은 "모델을 잘 만드는 것"과 "모델을 잘 쓰는 것"을 명확히 구분하면서도 두 영역을 하나의 책에서 연결해 설명한다는 점입니다. 대부분의 LLM 책이 둘 중 하나에만 집중하는 경향이 있는데, 이 책은 E2E 흐름 전체를 다룹니다.

아쉬운 점이 있다면, 2024년 7월 출간 시점 이후 LLM 생태계가 빠르게 변해 일부 도구와 프레임워크의 내용이 이미 구버전이 됐다는 것입니다. 하지만 이는 이 분야 책의 숙명이기도 하고, 기술보다는 원리를 중심으로 설명하는 저자의 방식 덕분에 핵심 내용은 여전히 유효합니다.

LLM을 처음 공부하는 분, 혹은 개발 경험은 있지만 LLM의 내부 동작이 궁금한 분 모두에게 권하는 책입니다.

---

**도서 정보**
- 저자: 허정준 지음 | 정진호 그림 | 박재호 감수
- 출판사: 책만
- 출간일: 2024년 7월 25일
- ISBN: 9791189909703
- 교보문고: https://product.kyobobook.co.kr/detail/S000213834592
- 실습 코드: https://github.com/onlybooks/llm
